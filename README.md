# ada-resnet
Adaptive ResNet approach. This approach is inspired by the StyleGAN paper, where latent
code information is inserted into multiple places in the CNN network. I attempt to recreate
this by resetting the batchnorm affine parameters and replacing them with affine
parameters generated by a linear transform of a writer code. Obtaining the writer code
is done the same way as in Abdel-Hamid et al. (2013), i.e. embeddings are trained for
each writer at train time, and randomly initialized followed by a few gradient steps on
a support batch at test time.

So far the approach doesn't really seem to work. This seems to have something to do with
the fact that changing the batchnorm affine parameters can have major consequences for
model performance. I experimented with some parameters such as freezing (entire base
model or only the decoder or no freezing), and batchnorm (resetting the affine
parameters, keeping them the same, freezing running statistics), but nothing yet is
promising. Note that in the StyleGAN paper they do not use batchnorm, which means they
can circumvent this problem altogether.

# Master thesis

## How to install
```shell
git@github.com:tobiasvanderwerff/master-thesis.git  # uses SSH
cd master-thesis
git submodule update --init
pip install -e htr
pip install -e .
```

## Points of attention
- 16-bit mixed precision cannot be used right now in combination with the
  `learn2learn` lib. This is because the `learn2learn` lib calls backpropagation
  for you when calling the inner loop adaptation function. This means the Pytorch
  Lightning cannot scale the gradients etc. accordingly when doing backpropagation.
